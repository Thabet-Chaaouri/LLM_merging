# LLM_merging

[Arcee's Mergekit](https://github.com/arcee-ai/mergekit) library and the different merging methods: https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html

What is [Mixture of experts](https://huggingface.co/blog/moe#what-is-a-mixture-of-experts-moe) ?
You can train a Mixture of experts like Mixtral or create a MoE model by merging different models : https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html .
